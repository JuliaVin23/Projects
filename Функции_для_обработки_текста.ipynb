{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLrBto78TWImYTWOUpLHi2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliaVin23/Projects/blob/main/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Создание функций для обработки текста при помощи ООП\n",
        "##Извлечение именованных сущностей, лемматизация и разметка частей речи"
      ],
      "metadata": {
        "id": "AUDOeoLb9nii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NERStats"
      ],
      "metadata": {
        "id": "Ot1AnT7hhMtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUlABaqrPWFT",
        "outputId": "3a899ced-6ca4-4c11-8a72-3a88adeab178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-19 15:53:26.322135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-19 15:53:26.322202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-19 15:53:26.324382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-19 15:53:26.337702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-19 15:53:31.740233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "qgaKsc-ox9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERExtractor:\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "    self.named_entities = []\n",
        "\n",
        "  def extract_named_entities(self, text):\n",
        "    doc = self.nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "  def display_named_entities(self):\n",
        "    for text, label in self.named_entities:\n",
        "      print(f\"Named entities: {text} - {label}\")\n",
        "\n",
        "\n",
        "ner_extractor = NERExtractor()\n",
        "ner_extractor.named_entities = ner_extractor.extract_named_entities(\"Apple Inc. is a technology company.\")\n",
        "ner_extractor.display_named_entities()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOoaiMJKzG9",
        "outputId": "396eff31-13f1-4c00-8696-af928897ec02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: Apple Inc. - ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re # regex - библиотека для использования регулярных выражений\n",
        "\n",
        "def remove_punctuation(input_string):\n",
        "    result_string = re.sub(r'[^\\w\\s]', '', input_string)\n",
        "    return result_string\n",
        "\n",
        "class NERStats:\n",
        "    def __init__(self, result_string):\n",
        "        self.result_string = result_string\n",
        "        self.ner_extractor = NERExtractor()\n",
        "        self.ner_frequencies = self._analyze_entities()\n",
        "\n",
        "    def _analyze_entities(self):\n",
        "        ner_extractor = self.ner_extractor.extract_named_entities(self.result_string)\n",
        "        ner_frequencies = Counter(ner_extractor)\n",
        "        return ner_frequencies\n",
        "\n",
        "    def display_most_common_entities(self, n):\n",
        "        most_common_entities = self.ner_frequencies.most_common(n)\n",
        "        print(f\"Top {n} most common entities:\")\n",
        "        for word, frequency in most_common_entities:\n",
        "            print(f\"{word}: {frequency}\")\n",
        "\n",
        "text_analyzer = NERStats(\"Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977.\")\n",
        "text_analyzer.display_most_common_entities(2)\n",
        "\n",
        "text_analyzer = NERStats(\"Apple is an American company. The US made Apple. Apple phones are very popular in the US.\")\n",
        "text_analyzer.display_most_common_entities(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUpUKrxjpWxM",
        "outputId": "4b93b995-6823-49e6-b780-7eccadd9a1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 2 most common entities:\n",
            "('Apple', 'ORG'): 2\n",
            "('Apple Computer Company', 'ORG'): 1\n",
            "Top 2 most common entities:\n",
            "('Apple', 'ORG'): 3\n",
            "('US', 'GPE'): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LemmaStats"
      ],
      "metadata": {
        "id": "jLoCxsQVqkLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re # regex - библиотека для использования регулярных выражений\n",
        "\n",
        "def remove_punctuation(input_string):\n",
        "    result_string = re.sub(r'[^\\w\\s]', '', input_string)\n",
        "    return result_string\n",
        "\n",
        "class LemmaStats:\n",
        "  def __init__(self, result_string):\n",
        "    import spacy\n",
        "    self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "    self.result_string = result_string\n",
        "    self.lemma_frequencies = self.lemmatize()\n",
        "\n",
        "  def lemmatize(self):\n",
        "    doc = self.nlp(self.result_string)\n",
        "    lemmatized_text = [token.lemma_ for token in doc]\n",
        "    lemma_frequencies = Counter(lemmatized_text)\n",
        "    return lemma_frequencies\n",
        "\n",
        "  def display_most_common_lemmas(self, n):\n",
        "    most_common_lemmas = self.lemma_frequencies.most_common(n)\n",
        "    print(f\"Top {n} most common lemmas:\")\n",
        "    for word, frequency in most_common_lemmas:\n",
        "        print(f\"{word}: {frequency}\")\n",
        "\n",
        "input_string = 'Python is a very common programming language. I am a user of Python language. Many people who speak different languages are using Python right now.'\n",
        "result_string = remove_punctuation(input_string)\n",
        "\n",
        "text_analyzer = LemmaStats(result_string)\n",
        "text_analyzer.display_most_common_lemmas(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Ae3SyTqu2j",
        "outputId": "38137f3c-faa2-4103-c065-ced72aa85b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 most common lemmas:\n",
            "Python: 3\n",
            "be: 3\n",
            "language: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PoSStats"
      ],
      "metadata": {
        "id": "RZxJlZiywJK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "EnoKNaezwMwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re # regex - библиотека для использования регулярных выражений\n",
        "\n",
        "def remove_punctuation(input_string):\n",
        "    result_string = re.sub(r'[^\\w\\s]', '', input_string)\n",
        "    return result_string\n",
        "\n",
        "\n",
        "class PoSStats:\n",
        "  def __init__(self, result_string):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    self.result_string = result_string\n",
        "    self.tagged_text = self.tokenize_and_tag()\n",
        "\n",
        "  def tokenize_and_tag(self):\n",
        "    tagged_text = nltk.pos_tag(word_tokenize(self.result_string))\n",
        "    return tagged_text\n",
        "\n",
        "  def display_most_common_pos(self, n):\n",
        "    pos_list = list(map(lambda x: x[1], self.tagged_text))\n",
        "    pos_frequencies = Counter(pos_list)\n",
        "    most_common_pos = pos_frequencies.most_common(n)\n",
        "    print(f\"Top {n} most common pos:\")\n",
        "    for pos, frequency in most_common_pos:\n",
        "          print(f\"{pos}: {frequency}\")\n",
        "\n",
        "input_string = 'Python is a very common programming language. I am a user of Python language. Many people who speak different languages are using Python right now.'\n",
        "result_string = remove_punctuation(input_string)\n",
        "\n",
        "text_analyzer = PoSStats(result_string)\n",
        "text_analyzer.display_most_common_pos(3)"
      ],
      "metadata": {
        "outputId": "12b4baa6-ce7a-41b4-eedb-d5c2989dfc08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxjg4_635oqH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 most common pos:\n",
            "NN: 5\n",
            "NNP: 3\n",
            "JJ: 3\n"
          ]
        }
      ]
    }
  ]
}